{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from data import html_code_list  # Import the array from data.py\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove BOM and invisible characters, and strip whitespace.\"\"\"\n",
        "    return text.replace('\\ufeff', '').strip()\n",
        "\n",
        "def extract_info(html):\n",
        "    \"\"\"Extracts information from a single HTML document.\"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    data = {}\n",
        "\n",
        "    # Place\n",
        "    place_tag = soup.find(\"h3\", class_=\"text-raven dark:text-indigo-100 text-base mt-2 md:mt-0 hover:underline cursor-pointer\")\n",
        "    data['place'] = clean_text(place_tag.get_text()) if place_tag else \"\"\n",
        "\n",
        "    # Scheme name\n",
        "    title_tag = soup.find(\"h1\", class_=\"font-bold text-xl sm:text-2xl text-[#24262B] dark:text-white mt-1\")\n",
        "    data['Scheme Name'] = clean_text(title_tag.get_text()) if title_tag else \"\"\n",
        "\n",
        "    # Details\n",
        "    details = []\n",
        "    details_section = soup.find(\"div\", id=\"details\")\n",
        "    if details_section:\n",
        "        for tag in details_section.find_all(['li', 'span']):\n",
        "            text = clean_text(tag.get_text())\n",
        "            if text and text not in details:\n",
        "                details.append(text)\n",
        "    data['Details'] = details\n",
        "\n",
        "    # Benefits\n",
        "    benefits = []\n",
        "    benefits_section = soup.find(\"div\", id=\"benefits\")\n",
        "    if benefits_section:\n",
        "        for tag in benefits_section.find_all(['li', 'span']):\n",
        "            text = clean_text(tag.get_text())\n",
        "            if text and text not in benefits:\n",
        "                benefits.append(text)\n",
        "    data['Benefits'] = benefits\n",
        "\n",
        "    # Eligibility\n",
        "    eligibility = []\n",
        "    eligibility_section = soup.find(\"div\", id=\"eligibility\")\n",
        "    if eligibility_section:\n",
        "        for tag in eligibility_section.find_all(['li', 'span']):\n",
        "            text = clean_text(tag.get_text())\n",
        "            if text and text not in eligibility:\n",
        "                eligibility.append(text)\n",
        "    data['Eligibility'] = eligibility\n",
        "\n",
        "    # Application Process\n",
        "    application_process = {}\n",
        "    application_section = soup.find(\"div\", id=\"application-process\")\n",
        "    if application_section:\n",
        "        tab_containers = application_section.find_all(\"div\", class_=\"rounded\")\n",
        "        for tab in tab_containers:\n",
        "            parent = tab.find_previous(\"div\", class_=\"overflow-x-auto\")\n",
        "            label = clean_text(parent.get_text()) if parent else \"Unknown\"\n",
        "\n",
        "            content = []\n",
        "            markdown = tab.find(\"div\", class_=\"markdown-options\")\n",
        "            if markdown:\n",
        "                for div in markdown.find_all(\"div\", class_=\"mb-2\"):\n",
        "                    text = clean_text(div.get_text(separator=\" \"))\n",
        "                    if text:\n",
        "                        content.append(text)\n",
        "\n",
        "            if label and content:\n",
        "                application_process[label] = content\n",
        "    data['Application Process'] = application_process\n",
        "\n",
        "    # Documents Required\n",
        "    documents_required = []\n",
        "    documents_section = soup.find(\"div\", id=\"documents-required\")\n",
        "    if documents_section:\n",
        "        for li in documents_section.find_all(\"li\"):\n",
        "            text = clean_text(li.get_text())\n",
        "            if text:\n",
        "                documents_required.append(text)\n",
        "    data['Documents Required'] = documents_required\n",
        "\n",
        "    # Exclusions\n",
        "    exclusions = []\n",
        "    exclusions_section = soup.find(\"div\", id=\"exclusions\")\n",
        "    if exclusions_section:\n",
        "        blockquotes = exclusions_section.find_all(\"blockquote\")\n",
        "        for bq in blockquotes:\n",
        "            text = clean_text(bq.get_text())\n",
        "            if text:\n",
        "                exclusions.append(text)\n",
        "    data['Exclusions'] = exclusions\n",
        "\n",
        "    # FAQs\n",
        "    faq_dict = {}\n",
        "    faq_section = soup.find(\"div\", id=\"faqs\")\n",
        "    if faq_section:\n",
        "        faq_items = faq_section.find_all(\"div\", class_=\"py-4\")\n",
        "        for item in faq_items:\n",
        "            question_tag = item.find(\"p\", class_=\"font-bold\")\n",
        "            question = clean_text(question_tag.get_text()) if question_tag else \"\"\n",
        "\n",
        "            answer_tag = item.find(\"div\", class_=\"rounded-b\")\n",
        "            answer_text = clean_text(answer_tag.get_text(separator=\" \")) if answer_tag else \"\"\n",
        "\n",
        "            if question and answer_text:\n",
        "                faq_dict[question] = answer_text\n",
        "    data['FAQs'] = faq_dict\n",
        "\n",
        "    # Sources and References\n",
        "    sources = []\n",
        "    sources_section = soup.find(\"div\", id=\"sources\")\n",
        "    if sources_section:\n",
        "        links = sources_section.find_all(\"a\", href=True)\n",
        "        for link in links:\n",
        "            text_tag = link.find(\"p\")\n",
        "            link_text = clean_text(text_tag.get_text()) if text_tag else clean_text(link.get_text())\n",
        "            url = link['href']\n",
        "            if link_text and url:\n",
        "                sources.append({\"text\": link_text, \"url\": url})\n",
        "    data['Sources and References'] = sources\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# ==== Main Execution Starts Here ====\n",
        "\n",
        "all_data = [extract_info(html) for html in html_code_list]\n",
        "\n",
        "# Save the data to a file\n",
        "with open(\"extracted_schemes.json\", \"w\", encoding='utf-8') as f:\n",
        "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✅ Extraction complete. Data saved to extracted_schemes.json.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M9Bt7flN_gJ",
        "outputId": "c0604081-167b-4082-cae9-7f9edb187d7e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete. Data saved to extracted_schemes.json.\n"
          ]
        }
      ]
    }
  ]
}